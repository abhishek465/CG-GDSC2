{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\Abhishek\\Downloads\\p.csv')\n",
    "\n",
    "y=np.array(data.test, dtype=np.float32)\n",
    "data = data.drop(['test'], 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.05, random_state=777)\n",
    "X_train=np.array(X_train, dtype=np.float32)\n",
    "X_test=np.array(X_test, dtype=np.float32)\n",
    "#X_val=np.array(X_val, dtype=np.float32)\n",
    "\n",
    "y_train=np.array(y_train, dtype=np.float32)\n",
    "y_test=np.array(y_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgbm_pred</th>\n",
       "      <th>xg_pred</th>\n",
       "      <th>knn_base</th>\n",
       "      <th>pred1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.074791</td>\n",
       "      <td>4.570628</td>\n",
       "      <td>2.945541</td>\n",
       "      <td>6.363816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.942582</td>\n",
       "      <td>2.683648</td>\n",
       "      <td>2.936274</td>\n",
       "      <td>5.286103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.898725</td>\n",
       "      <td>2.038235</td>\n",
       "      <td>2.933912</td>\n",
       "      <td>4.918706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.898725</td>\n",
       "      <td>3.420010</td>\n",
       "      <td>2.936342</td>\n",
       "      <td>5.674343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.982889</td>\n",
       "      <td>3.992559</td>\n",
       "      <td>3.007710</td>\n",
       "      <td>6.083176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lgbm_pred   xg_pred  knn_base     pred1\n",
       "0   3.074791  4.570628  2.945541  6.363816\n",
       "1   2.942582  2.683648  2.936274  5.286103\n",
       "2   2.898725  2.038235  2.933912  4.918706\n",
       "3   2.898725  3.420010  2.936342  5.674343\n",
       "4   2.982889  3.992559  3.007710  6.083176"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def rmse_keras(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense( 128, input_dim=4))\n",
    "model.add(LeakyReLU(alpha=0.6812890283499637))\n",
    "model.add(Dropout(0.5996055791654191))\n",
    "model.add(layers.Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.6966810108234018))\n",
    "model.add(Dropout(0.5000396099125821))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.001)\n",
    "\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    \n",
    "model.compile(optimizer='rmsprop',loss=rmse_keras,metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 390 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "390/390 [==============================] - 1s 3ms/step - loss: 2.3818 - mean_squared_error: 6.0515 - val_loss: 0.8252 - val_mean_squared_error: 0.6957\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 0s 246us/step - loss: 2.0311 - mean_squared_error: 4.2409 - val_loss: 1.1559 - val_mean_squared_error: 1.3590\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 0s 272us/step - loss: 1.9087 - mean_squared_error: 3.7296 - val_loss: 0.7626 - val_mean_squared_error: 0.5930\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 0s 253us/step - loss: 1.7080 - mean_squared_error: 3.0239 - val_loss: 0.8889 - val_mean_squared_error: 0.8056\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 0s 242us/step - loss: 1.5904 - mean_squared_error: 2.5999 - val_loss: 0.8201 - val_mean_squared_error: 0.6857\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 0s 252us/step - loss: 1.4332 - mean_squared_error: 2.1053 - val_loss: 0.5841 - val_mean_squared_error: 0.3442\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 0s 253us/step - loss: 1.4348 - mean_squared_error: 2.1110 - val_loss: 0.6523 - val_mean_squared_error: 0.4318\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 0s 248us/step - loss: 1.4384 - mean_squared_error: 2.1304 - val_loss: 0.5496 - val_mean_squared_error: 0.3053\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 0s 256us/step - loss: 1.3483 - mean_squared_error: 1.8592 - val_loss: 0.7920 - val_mean_squared_error: 0.6388\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 0s 234us/step - loss: 1.2741 - mean_squared_error: 1.6986 - val_loss: 0.6250 - val_mean_squared_error: 0.3959\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 0s 253us/step - loss: 1.2872 - mean_squared_error: 1.7075 - val_loss: 0.5445 - val_mean_squared_error: 0.3058\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 0s 276us/step - loss: 1.3438 - mean_squared_error: 1.8551 - val_loss: 0.7674 - val_mean_squared_error: 0.5986\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 0s 289us/step - loss: 1.2182 - mean_squared_error: 1.5236 - val_loss: 0.5169 - val_mean_squared_error: 0.2716\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 0s 276us/step - loss: 1.2294 - mean_squared_error: 1.5508 - val_loss: 0.5139 - val_mean_squared_error: 0.2696\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 0s 287us/step - loss: 1.2011 - mean_squared_error: 1.4788 - val_loss: 0.7182 - val_mean_squared_error: 0.5234\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 0s 302us/step - loss: 1.2198 - mean_squared_error: 1.5254 - val_loss: 0.5183 - val_mean_squared_error: 0.2710\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 0s 288us/step - loss: 1.2279 - mean_squared_error: 1.5563 - val_loss: 0.7493 - val_mean_squared_error: 0.5696\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 0s 280us/step - loss: 1.1861 - mean_squared_error: 1.4455 - val_loss: 0.5400 - val_mean_squared_error: 0.2946\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 0s 266us/step - loss: 1.2373 - mean_squared_error: 1.5883 - val_loss: 0.6431 - val_mean_squared_error: 0.4187\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 0s 248us/step - loss: 1.2395 - mean_squared_error: 1.5671 - val_loss: 0.5665 - val_mean_squared_error: 0.3242\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 0s 333us/step - loss: 1.1658 - mean_squared_error: 1.4194 - val_loss: 0.6774 - val_mean_squared_error: 0.4648\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 0s 287us/step - loss: 1.1319 - mean_squared_error: 1.3332 - val_loss: 0.6820 - val_mean_squared_error: 0.4709\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 0s 223us/step - loss: 1.1211 - mean_squared_error: 1.2945 - val_loss: 0.6096 - val_mean_squared_error: 0.3758\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 0s 225us/step - loss: 1.0555 - mean_squared_error: 1.1695 - val_loss: 0.6816 - val_mean_squared_error: 0.4701\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 0s 245us/step - loss: 1.0706 - mean_squared_error: 1.1873 - val_loss: 0.6390 - val_mean_squared_error: 0.4128\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 0s 266us/step - loss: 1.1092 - mean_squared_error: 1.2874 - val_loss: 0.8132 - val_mean_squared_error: 0.6687\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 0s 281us/step - loss: 1.1067 - mean_squared_error: 1.2784 - val_loss: 0.5461 - val_mean_squared_error: 0.3013\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 0s 255us/step - loss: 1.1066 - mean_squared_error: 1.2510 - val_loss: 0.6487 - val_mean_squared_error: 0.4259\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 0s 234us/step - loss: 1.1262 - mean_squared_error: 1.3101 - val_loss: 0.6109 - val_mean_squared_error: 0.3771\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 0s 226us/step - loss: 1.0645 - mean_squared_error: 1.1579 - val_loss: 0.6036 - val_mean_squared_error: 0.3680\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 0s 232us/step - loss: 1.0671 - mean_squared_error: 1.1780 - val_loss: 0.4860 - val_mean_squared_error: 0.2389\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 0s 229us/step - loss: 1.0519 - mean_squared_error: 1.1494 - val_loss: 0.5338 - val_mean_squared_error: 0.2879\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 0s 229us/step - loss: 1.0267 - mean_squared_error: 1.0873 - val_loss: 0.8093 - val_mean_squared_error: 0.6617\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 0s 231us/step - loss: 1.0813 - mean_squared_error: 1.2184 - val_loss: 0.4783 - val_mean_squared_error: 0.2319\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 0s 212us/step - loss: 1.1043 - mean_squared_error: 1.2698 - val_loss: 0.6085 - val_mean_squared_error: 0.3748\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 0s 216us/step - loss: 1.0456 - mean_squared_error: 1.1187 - val_loss: 0.5152 - val_mean_squared_error: 0.2755\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 0s 233us/step - loss: 1.0078 - mean_squared_error: 1.0432 - val_loss: 0.5189 - val_mean_squared_error: 0.2719\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 0s 225us/step - loss: 1.0086 - mean_squared_error: 1.0511 - val_loss: 0.6808 - val_mean_squared_error: 0.4682\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 0s 241us/step - loss: 1.0783 - mean_squared_error: 1.1829 - val_loss: 0.5193 - val_mean_squared_error: 0.2723\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 0s 222us/step - loss: 1.0662 - mean_squared_error: 1.1725 - val_loss: 0.7278 - val_mean_squared_error: 0.5353\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 0s 213us/step - loss: 0.9568 - mean_squared_error: 0.9636 - val_loss: 0.5010 - val_mean_squared_error: 0.2606\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 0s 216us/step - loss: 1.0882 - mean_squared_error: 1.2351 - val_loss: 0.4744 - val_mean_squared_error: 0.2292\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 0s 245us/step - loss: 0.9965 - mean_squared_error: 1.0266 - val_loss: 0.4945 - val_mean_squared_error: 0.2471\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 0s 218us/step - loss: 0.9765 - mean_squared_error: 0.9855 - val_loss: 0.6299 - val_mean_squared_error: 0.4022\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 0s 230us/step - loss: 1.0428 - mean_squared_error: 1.1252 - val_loss: 0.5210 - val_mean_squared_error: 0.2741\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 0s 235us/step - loss: 1.0366 - mean_squared_error: 1.0989 - val_loss: 0.5461 - val_mean_squared_error: 0.3011\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 0s 229us/step - loss: 1.0401 - mean_squared_error: 1.1284 - val_loss: 0.5341 - val_mean_squared_error: 0.2879\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 0s 227us/step - loss: 1.0523 - mean_squared_error: 1.1579 - val_loss: 0.5110 - val_mean_squared_error: 0.2641\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 0s 232us/step - loss: 1.0251 - mean_squared_error: 1.1007 - val_loss: 0.6282 - val_mean_squared_error: 0.3984\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 0s 240us/step - loss: 0.9761 - mean_squared_error: 0.9984 - val_loss: 0.4816 - val_mean_squared_error: 0.2349\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 0s 252us/step - loss: 1.0341 - mean_squared_error: 1.0960 - val_loss: 0.5882 - val_mean_squared_error: 0.3501\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 0s 261us/step - loss: 1.0673 - mean_squared_error: 1.1924 - val_loss: 0.5459 - val_mean_squared_error: 0.3009\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 0s 240us/step - loss: 0.9566 - mean_squared_error: 0.9397 - val_loss: 0.4846 - val_mean_squared_error: 0.2377\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 0s 315us/step - loss: 0.9730 - mean_squared_error: 0.9597 - val_loss: 0.4754 - val_mean_squared_error: 0.2296\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 0s 381us/step - loss: 0.9756 - mean_squared_error: 0.9700 - val_loss: 0.6497 - val_mean_squared_error: 0.4259\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 0s 263us/step - loss: 0.9953 - mean_squared_error: 1.0359 - val_loss: 0.7679 - val_mean_squared_error: 0.5955\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 0s 258us/step - loss: 0.9628 - mean_squared_error: 0.9590 - val_loss: 0.4721 - val_mean_squared_error: 0.2272\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 0s 226us/step - loss: 1.0153 - mean_squared_error: 1.0813 - val_loss: 0.5171 - val_mean_squared_error: 0.2701\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 0s 227us/step - loss: 1.0163 - mean_squared_error: 1.0755 - val_loss: 0.5308 - val_mean_squared_error: 0.2844\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 0s 257us/step - loss: 1.0074 - mean_squared_error: 1.0390 - val_loss: 0.5004 - val_mean_squared_error: 0.2583\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 0s 254us/step - loss: 0.9524 - mean_squared_error: 0.9424 - val_loss: 0.4787 - val_mean_squared_error: 0.2325\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 0s 335us/step - loss: 1.0420 - mean_squared_error: 1.1184 - val_loss: 0.4708 - val_mean_squared_error: 0.2260\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 0s 281us/step - loss: 0.9730 - mean_squared_error: 0.9793 - val_loss: 0.5069 - val_mean_squared_error: 0.2663\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 0s 235us/step - loss: 0.8957 - mean_squared_error: 0.8362 - val_loss: 0.4912 - val_mean_squared_error: 0.2447\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 0.9418 - mean_squared_error: 0.9355 - val_loss: 0.6665 - val_mean_squared_error: 0.4481\n",
      "Epoch 66/100\n",
      "390/390 [==============================] - 0s 226us/step - loss: 0.9977 - mean_squared_error: 1.0359 - val_loss: 0.4688 - val_mean_squared_error: 0.2248\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 0s 222us/step - loss: 1.0033 - mean_squared_error: 1.0397 - val_loss: 0.4719 - val_mean_squared_error: 0.2286\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 0s 235us/step - loss: 0.9756 - mean_squared_error: 0.9826 - val_loss: 0.4762 - val_mean_squared_error: 0.2342\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 0s 232us/step - loss: 0.9382 - mean_squared_error: 0.9200 - val_loss: 0.5071 - val_mean_squared_error: 0.2600\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 0s 234us/step - loss: 0.9818 - mean_squared_error: 1.0010 - val_loss: 0.5557 - val_mean_squared_error: 0.3117\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 0s 218us/step - loss: 1.0068 - mean_squared_error: 1.0451 - val_loss: 0.4886 - val_mean_squared_error: 0.2416\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 0s 238us/step - loss: 0.9330 - mean_squared_error: 0.8981 - val_loss: 0.4884 - val_mean_squared_error: 0.2414\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 0s 224us/step - loss: 0.9309 - mean_squared_error: 0.8981 - val_loss: 0.5941 - val_mean_squared_error: 0.3560\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 0s 242us/step - loss: 0.9119 - mean_squared_error: 0.8622 - val_loss: 0.5984 - val_mean_squared_error: 0.3708\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 0s 229us/step - loss: 0.8987 - mean_squared_error: 0.8327 - val_loss: 0.5843 - val_mean_squared_error: 0.3444\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 0s 230us/step - loss: 0.9206 - mean_squared_error: 0.8699 - val_loss: 0.6270 - val_mean_squared_error: 0.3965\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 0s 224us/step - loss: 0.9569 - mean_squared_error: 0.9352 - val_loss: 0.4885 - val_mean_squared_error: 0.2418\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 0s 226us/step - loss: 0.8953 - mean_squared_error: 0.8349 - val_loss: 0.4951 - val_mean_squared_error: 0.2484\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 0s 215us/step - loss: 0.9139 - mean_squared_error: 0.8608 - val_loss: 0.5612 - val_mean_squared_error: 0.3181\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 0s 218us/step - loss: 0.9795 - mean_squared_error: 1.0021 - val_loss: 0.4974 - val_mean_squared_error: 0.2566\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 0s 261us/step - loss: 0.9464 - mean_squared_error: 0.9265 - val_loss: 0.4724 - val_mean_squared_error: 0.2280\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 0s 248us/step - loss: 0.9291 - mean_squared_error: 0.8948 - val_loss: 0.6002 - val_mean_squared_error: 0.3634\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 0s 229us/step - loss: 0.8700 - mean_squared_error: 0.7748 - val_loss: 0.5841 - val_mean_squared_error: 0.3446\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 0s 261us/step - loss: 0.8603 - mean_squared_error: 0.7627 - val_loss: 0.4768 - val_mean_squared_error: 0.2323\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 0s 270us/step - loss: 0.9271 - mean_squared_error: 0.8789 - val_loss: 0.5317 - val_mean_squared_error: 0.2857\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 0s 307us/step - loss: 0.8945 - mean_squared_error: 0.8177 - val_loss: 0.5110 - val_mean_squared_error: 0.2639\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 0s 299us/step - loss: 0.9113 - mean_squared_error: 0.8547 - val_loss: 0.4983 - val_mean_squared_error: 0.2518\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 0s 276us/step - loss: 0.9084 - mean_squared_error: 0.8499 - val_loss: 0.6319 - val_mean_squared_error: 0.4027\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 0s 280us/step - loss: 0.8951 - mean_squared_error: 0.8224 - val_loss: 0.4784 - val_mean_squared_error: 0.2356\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 0s 274us/step - loss: 0.8610 - mean_squared_error: 0.7654 - val_loss: 0.4767 - val_mean_squared_error: 0.2318\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 0s 287us/step - loss: 0.8997 - mean_squared_error: 0.8497 - val_loss: 0.4848 - val_mean_squared_error: 0.2435\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 0s 245us/step - loss: 0.8294 - mean_squared_error: 0.7133 - val_loss: 0.7032 - val_mean_squared_error: 0.4991\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 0s 230us/step - loss: 0.8928 - mean_squared_error: 0.8354 - val_loss: 0.5669 - val_mean_squared_error: 0.3245\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 0s 229us/step - loss: 0.8689 - mean_squared_error: 0.7772 - val_loss: 0.4790 - val_mean_squared_error: 0.2361\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 0s 224us/step - loss: 0.8899 - mean_squared_error: 0.8108 - val_loss: 0.6569 - val_mean_squared_error: 0.4352\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 0s 222us/step - loss: 0.8756 - mean_squared_error: 0.7897 - val_loss: 0.5614 - val_mean_squared_error: 0.3262\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 0s 243us/step - loss: 0.8708 - mean_squared_error: 0.7774 - val_loss: 0.4731 - val_mean_squared_error: 0.2294\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 0s 238us/step - loss: 0.8203 - mean_squared_error: 0.6928 - val_loss: 0.4985 - val_mean_squared_error: 0.2516\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 0s 227us/step - loss: 0.8824 - mean_squared_error: 0.7986 - val_loss: 0.4925 - val_mean_squared_error: 0.2521\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 0s 220us/step - loss: 0.8505 - mean_squared_error: 0.7494 - val_loss: 0.4761 - val_mean_squared_error: 0.2328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2214df85358>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.25, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input4\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    }
   ],
   "source": [
    "col=['lgbm_pred', 'xg_pred', 'knn_base', 'pred1']\n",
    "data['p']=np.clip(model.predict(data[col]),y.min(),y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgbm_pred</th>\n",
       "      <th>xg_pred</th>\n",
       "      <th>knn_base</th>\n",
       "      <th>pred1</th>\n",
       "      <th>p</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.074791</td>\n",
       "      <td>4.570628</td>\n",
       "      <td>2.945541</td>\n",
       "      <td>6.363816</td>\n",
       "      <td>6.431535</td>\n",
       "      <td>7.015641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.942582</td>\n",
       "      <td>2.683648</td>\n",
       "      <td>2.936274</td>\n",
       "      <td>5.286103</td>\n",
       "      <td>5.995590</td>\n",
       "      <td>6.984855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.898725</td>\n",
       "      <td>2.038235</td>\n",
       "      <td>2.933912</td>\n",
       "      <td>4.918706</td>\n",
       "      <td>5.843705</td>\n",
       "      <td>6.951619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.898725</td>\n",
       "      <td>3.420010</td>\n",
       "      <td>2.936342</td>\n",
       "      <td>5.674343</td>\n",
       "      <td>6.256819</td>\n",
       "      <td>6.942707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.982889</td>\n",
       "      <td>3.992559</td>\n",
       "      <td>3.007710</td>\n",
       "      <td>6.083176</td>\n",
       "      <td>6.484743</td>\n",
       "      <td>6.940397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.819722</td>\n",
       "      <td>3.739284</td>\n",
       "      <td>2.903941</td>\n",
       "      <td>5.792165</td>\n",
       "      <td>6.359207</td>\n",
       "      <td>6.899784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.942582</td>\n",
       "      <td>3.071655</td>\n",
       "      <td>2.870377</td>\n",
       "      <td>5.431709</td>\n",
       "      <td>5.978416</td>\n",
       "      <td>6.901687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.982889</td>\n",
       "      <td>3.564520</td>\n",
       "      <td>3.007426</td>\n",
       "      <td>5.849568</td>\n",
       "      <td>6.361326</td>\n",
       "      <td>6.929282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.103274</td>\n",
       "      <td>3.472375</td>\n",
       "      <td>3.010175</td>\n",
       "      <td>5.838376</td>\n",
       "      <td>6.224031</td>\n",
       "      <td>6.909733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.279340</td>\n",
       "      <td>3.914697</td>\n",
       "      <td>2.893709</td>\n",
       "      <td>6.016091</td>\n",
       "      <td>5.949756</td>\n",
       "      <td>6.902733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.210430</td>\n",
       "      <td>3.683119</td>\n",
       "      <td>3.047006</td>\n",
       "      <td>6.022383</td>\n",
       "      <td>6.257767</td>\n",
       "      <td>6.872946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.006790</td>\n",
       "      <td>2.882577</td>\n",
       "      <td>2.995837</td>\n",
       "      <td>5.473456</td>\n",
       "      <td>6.112426</td>\n",
       "      <td>6.860486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.915818</td>\n",
       "      <td>3.010146</td>\n",
       "      <td>2.890292</td>\n",
       "      <td>5.410027</td>\n",
       "      <td>6.025813</td>\n",
       "      <td>6.788070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.823916</td>\n",
       "      <td>2.471872</td>\n",
       "      <td>3.008294</td>\n",
       "      <td>5.206914</td>\n",
       "      <td>6.191506</td>\n",
       "      <td>6.768493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.744913</td>\n",
       "      <td>3.338465</td>\n",
       "      <td>2.979655</td>\n",
       "      <td>5.626843</td>\n",
       "      <td>6.467149</td>\n",
       "      <td>6.737109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.744913</td>\n",
       "      <td>3.338465</td>\n",
       "      <td>2.981427</td>\n",
       "      <td>5.628614</td>\n",
       "      <td>6.470703</td>\n",
       "      <td>6.737097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.823916</td>\n",
       "      <td>3.418502</td>\n",
       "      <td>2.992081</td>\n",
       "      <td>5.706710</td>\n",
       "      <td>6.440020</td>\n",
       "      <td>6.742951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.951938</td>\n",
       "      <td>3.852882</td>\n",
       "      <td>2.849683</td>\n",
       "      <td>5.839682</td>\n",
       "      <td>6.155368</td>\n",
       "      <td>6.643777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.990627</td>\n",
       "      <td>2.812818</td>\n",
       "      <td>2.867270</td>\n",
       "      <td>5.301991</td>\n",
       "      <td>5.849503</td>\n",
       "      <td>6.630288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.990627</td>\n",
       "      <td>2.793758</td>\n",
       "      <td>2.895620</td>\n",
       "      <td>5.319951</td>\n",
       "      <td>5.900691</td>\n",
       "      <td>6.592195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.074791</td>\n",
       "      <td>3.363286</td>\n",
       "      <td>2.882057</td>\n",
       "      <td>5.642208</td>\n",
       "      <td>5.961914</td>\n",
       "      <td>6.627777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.942582</td>\n",
       "      <td>2.465449</td>\n",
       "      <td>2.898407</td>\n",
       "      <td>5.129295</td>\n",
       "      <td>5.855447</td>\n",
       "      <td>6.581928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.942582</td>\n",
       "      <td>2.961214</td>\n",
       "      <td>2.899778</td>\n",
       "      <td>5.400908</td>\n",
       "      <td>6.004624</td>\n",
       "      <td>6.624437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.735558</td>\n",
       "      <td>3.052878</td>\n",
       "      <td>2.899550</td>\n",
       "      <td>5.388244</td>\n",
       "      <td>6.230300</td>\n",
       "      <td>6.571233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.105749</td>\n",
       "      <td>2.320447</td>\n",
       "      <td>2.949216</td>\n",
       "      <td>5.150246</td>\n",
       "      <td>5.758554</td>\n",
       "      <td>6.647740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.026746</td>\n",
       "      <td>4.161148</td>\n",
       "      <td>2.918351</td>\n",
       "      <td>6.098936</td>\n",
       "      <td>6.309538</td>\n",
       "      <td>6.643855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.026746</td>\n",
       "      <td>4.178300</td>\n",
       "      <td>2.914075</td>\n",
       "      <td>6.104010</td>\n",
       "      <td>6.305638</td>\n",
       "      <td>6.595657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.946769</td>\n",
       "      <td>3.284321</td>\n",
       "      <td>2.936367</td>\n",
       "      <td>5.614884</td>\n",
       "      <td>6.170079</td>\n",
       "      <td>6.641339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.946769</td>\n",
       "      <td>3.568270</td>\n",
       "      <td>2.972508</td>\n",
       "      <td>5.805806</td>\n",
       "      <td>6.326720</td>\n",
       "      <td>6.589036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.942582</td>\n",
       "      <td>2.959360</td>\n",
       "      <td>2.949516</td>\n",
       "      <td>5.449635</td>\n",
       "      <td>6.103843</td>\n",
       "      <td>6.618886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>1.737194</td>\n",
       "      <td>2.231776</td>\n",
       "      <td>4.125274</td>\n",
       "      <td>4.125912</td>\n",
       "      <td>4.216267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>3.096392</td>\n",
       "      <td>1.808953</td>\n",
       "      <td>2.229317</td>\n",
       "      <td>4.148710</td>\n",
       "      <td>4.181361</td>\n",
       "      <td>4.571613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>1.083295</td>\n",
       "      <td>2.201167</td>\n",
       "      <td>3.738223</td>\n",
       "      <td>3.858468</td>\n",
       "      <td>3.046425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>1.585307</td>\n",
       "      <td>2.220050</td>\n",
       "      <td>4.030754</td>\n",
       "      <td>4.057911</td>\n",
       "      <td>4.316821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>2.933225</td>\n",
       "      <td>0.870502</td>\n",
       "      <td>2.216705</td>\n",
       "      <td>3.575366</td>\n",
       "      <td>4.017590</td>\n",
       "      <td>3.292498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>3.096392</td>\n",
       "      <td>1.576642</td>\n",
       "      <td>2.228416</td>\n",
       "      <td>4.021177</td>\n",
       "      <td>4.113034</td>\n",
       "      <td>2.925310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>3.188294</td>\n",
       "      <td>1.179935</td>\n",
       "      <td>2.230577</td>\n",
       "      <td>3.834793</td>\n",
       "      <td>3.902620</td>\n",
       "      <td>2.554899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>1.684596</td>\n",
       "      <td>2.231776</td>\n",
       "      <td>4.096602</td>\n",
       "      <td>4.110738</td>\n",
       "      <td>4.205140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>3.188294</td>\n",
       "      <td>1.349242</td>\n",
       "      <td>2.231532</td>\n",
       "      <td>3.928039</td>\n",
       "      <td>3.960141</td>\n",
       "      <td>2.906354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>1.660757</td>\n",
       "      <td>2.217734</td>\n",
       "      <td>4.069566</td>\n",
       "      <td>4.075974</td>\n",
       "      <td>3.555634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>1.335807</td>\n",
       "      <td>2.217734</td>\n",
       "      <td>3.892435</td>\n",
       "      <td>3.974460</td>\n",
       "      <td>3.555634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2.933225</td>\n",
       "      <td>1.828162</td>\n",
       "      <td>2.219081</td>\n",
       "      <td>4.099763</td>\n",
       "      <td>4.321425</td>\n",
       "      <td>2.940220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>1.660757</td>\n",
       "      <td>2.217734</td>\n",
       "      <td>4.069566</td>\n",
       "      <td>4.075974</td>\n",
       "      <td>3.086943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>3.188294</td>\n",
       "      <td>1.685343</td>\n",
       "      <td>2.231532</td>\n",
       "      <td>4.111248</td>\n",
       "      <td>4.065493</td>\n",
       "      <td>2.551786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>3.188294</td>\n",
       "      <td>1.637450</td>\n",
       "      <td>2.219856</td>\n",
       "      <td>4.073465</td>\n",
       "      <td>4.028093</td>\n",
       "      <td>2.944439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>3.188294</td>\n",
       "      <td>1.320965</td>\n",
       "      <td>2.225035</td>\n",
       "      <td>3.906127</td>\n",
       "      <td>3.938105</td>\n",
       "      <td>2.982647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>3.303416</td>\n",
       "      <td>1.334954</td>\n",
       "      <td>2.227451</td>\n",
       "      <td>3.950869</td>\n",
       "      <td>3.838793</td>\n",
       "      <td>3.736717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>3.272458</td>\n",
       "      <td>2.638507</td>\n",
       "      <td>2.231532</td>\n",
       "      <td>4.656188</td>\n",
       "      <td>4.273751</td>\n",
       "      <td>4.200505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>3.272458</td>\n",
       "      <td>2.081249</td>\n",
       "      <td>2.229697</td>\n",
       "      <td>4.350590</td>\n",
       "      <td>4.100281</td>\n",
       "      <td>4.199905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>3.188294</td>\n",
       "      <td>1.826104</td>\n",
       "      <td>2.229697</td>\n",
       "      <td>4.186141</td>\n",
       "      <td>4.103286</td>\n",
       "      <td>4.199905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>3.188294</td>\n",
       "      <td>1.396145</td>\n",
       "      <td>2.229697</td>\n",
       "      <td>3.951770</td>\n",
       "      <td>3.972248</td>\n",
       "      <td>4.199455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>1.077349</td>\n",
       "      <td>2.219872</td>\n",
       "      <td>3.753688</td>\n",
       "      <td>3.893348</td>\n",
       "      <td>3.225255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>0.719565</td>\n",
       "      <td>2.215984</td>\n",
       "      <td>3.554771</td>\n",
       "      <td>3.760193</td>\n",
       "      <td>4.080245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>3.351461</td>\n",
       "      <td>0.894919</td>\n",
       "      <td>2.234120</td>\n",
       "      <td>3.732157</td>\n",
       "      <td>3.656545</td>\n",
       "      <td>2.655352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>3.188294</td>\n",
       "      <td>1.264700</td>\n",
       "      <td>2.223579</td>\n",
       "      <td>3.874001</td>\n",
       "      <td>3.916819</td>\n",
       "      <td>3.299534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>2.213665</td>\n",
       "      <td>3.167605</td>\n",
       "      <td>3.496198</td>\n",
       "      <td>3.306887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>0.570815</td>\n",
       "      <td>2.217718</td>\n",
       "      <td>3.475421</td>\n",
       "      <td>3.710740</td>\n",
       "      <td>4.129712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>0.613034</td>\n",
       "      <td>2.199424</td>\n",
       "      <td>3.480140</td>\n",
       "      <td>3.689600</td>\n",
       "      <td>2.583243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>3.140250</td>\n",
       "      <td>0.731698</td>\n",
       "      <td>2.217761</td>\n",
       "      <td>3.563162</td>\n",
       "      <td>3.768023</td>\n",
       "      <td>3.719409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>2.933225</td>\n",
       "      <td>0.792941</td>\n",
       "      <td>2.218204</td>\n",
       "      <td>3.534586</td>\n",
       "      <td>3.992988</td>\n",
       "      <td>3.319987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lgbm_pred   xg_pred  knn_base     pred1         p      test\n",
       "0     3.074791  4.570628  2.945541  6.363816  6.431535  7.015641\n",
       "1     2.942582  2.683648  2.936274  5.286103  5.995590  6.984855\n",
       "2     2.898725  2.038235  2.933912  4.918706  5.843705  6.951619\n",
       "3     2.898725  3.420010  2.936342  5.674343  6.256819  6.942707\n",
       "4     2.982889  3.992559  3.007710  6.083176  6.484743  6.940397\n",
       "5     2.819722  3.739284  2.903941  5.792165  6.359207  6.899784\n",
       "6     2.942582  3.071655  2.870377  5.431709  5.978416  6.901687\n",
       "7     2.982889  3.564520  3.007426  5.849568  6.361326  6.929282\n",
       "8     3.103274  3.472375  3.010175  5.838376  6.224031  6.909733\n",
       "9     3.279340  3.914697  2.893709  6.016091  5.949756  6.902733\n",
       "10    3.210430  3.683119  3.047006  6.022383  6.257767  6.872946\n",
       "11    3.006790  2.882577  2.995837  5.473456  6.112426  6.860486\n",
       "12    2.915818  3.010146  2.890292  5.410027  6.025813  6.788070\n",
       "13    2.823916  2.471872  3.008294  5.206914  6.191506  6.768493\n",
       "14    2.744913  3.338465  2.979655  5.626843  6.467149  6.737109\n",
       "15    2.744913  3.338465  2.981427  5.628614  6.470703  6.737097\n",
       "16    2.823916  3.418502  2.992081  5.706710  6.440020  6.742951\n",
       "17    2.951938  3.852882  2.849683  5.839682  6.155368  6.643777\n",
       "18    2.990627  2.812818  2.867270  5.301991  5.849503  6.630288\n",
       "19    2.990627  2.793758  2.895620  5.319951  5.900691  6.592195\n",
       "20    3.074791  3.363286  2.882057  5.642208  5.961914  6.627777\n",
       "21    2.942582  2.465449  2.898407  5.129295  5.855447  6.581928\n",
       "22    2.942582  2.961214  2.899778  5.400908  6.004624  6.624437\n",
       "23    2.735558  3.052878  2.899550  5.388244  6.230300  6.571233\n",
       "24    3.105749  2.320447  2.949216  5.150246  5.758554  6.647740\n",
       "25    3.026746  4.161148  2.918351  6.098936  6.309538  6.643855\n",
       "26    3.026746  4.178300  2.914075  6.104010  6.305638  6.595657\n",
       "27    2.946769  3.284321  2.936367  5.614884  6.170079  6.641339\n",
       "28    2.946769  3.568270  2.972508  5.805806  6.326720  6.589036\n",
       "29    2.942582  2.959360  2.949516  5.449635  6.103843  6.618886\n",
       "..         ...       ...       ...       ...       ...       ...\n",
       "518   3.140250  1.737194  2.231776  4.125274  4.125912  4.216267\n",
       "519   3.096392  1.808953  2.229317  4.148710  4.181361  4.571613\n",
       "520   3.140250  1.083295  2.201167  3.738223  3.858468  3.046425\n",
       "521   3.140250  1.585307  2.220050  4.030754  4.057911  4.316821\n",
       "522   2.933225  0.870502  2.216705  3.575366  4.017590  3.292498\n",
       "523   3.096392  1.576642  2.228416  4.021177  4.113034  2.925310\n",
       "524   3.188294  1.179935  2.230577  3.834793  3.902620  2.554899\n",
       "525   3.140250  1.684596  2.231776  4.096602  4.110738  4.205140\n",
       "526   3.188294  1.349242  2.231532  3.928039  3.960141  2.906354\n",
       "527   3.140250  1.660757  2.217734  4.069566  4.075974  3.555634\n",
       "528   3.140250  1.335807  2.217734  3.892435  3.974460  3.555634\n",
       "529   2.933225  1.828162  2.219081  4.099763  4.321425  2.940220\n",
       "530   3.140250  1.660757  2.217734  4.069566  4.075974  3.086943\n",
       "531   3.188294  1.685343  2.231532  4.111248  4.065493  2.551786\n",
       "532   3.188294  1.637450  2.219856  4.073465  4.028093  2.944439\n",
       "533   3.188294  1.320965  2.225035  3.906127  3.938105  2.982647\n",
       "534   3.303416  1.334954  2.227451  3.950869  3.838793  3.736717\n",
       "535   3.272458  2.638507  2.231532  4.656188  4.273751  4.200505\n",
       "536   3.272458  2.081249  2.229697  4.350590  4.100281  4.199905\n",
       "537   3.188294  1.826104  2.229697  4.186141  4.103286  4.199905\n",
       "538   3.188294  1.396145  2.229697  3.951770  3.972248  4.199455\n",
       "539   3.140250  1.077349  2.219872  3.753688  3.893348  3.225255\n",
       "540   3.140250  0.719565  2.215984  3.554771  3.760193  4.080245\n",
       "541   3.351461  0.894919  2.234120  3.732157  3.656545  2.655352\n",
       "542   3.188294  1.264700  2.223579  3.874001  3.916819  3.299534\n",
       "543   3.140250  0.013555  2.213665  3.167605  3.496198  3.306887\n",
       "544   3.140250  0.570815  2.217718  3.475421  3.710740  4.129712\n",
       "545   3.140250  0.613034  2.199424  3.480140  3.689600  2.583243\n",
       "546   3.140250  0.731698  2.217761  3.563162  3.768023  3.719409\n",
       "547   2.933225  0.792941  2.218204  3.534586  3.992988  3.319987\n",
       "\n",
       "[548 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test']=y\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44939551814619705"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(data.p,data.test)#0.5081456192168586#0.44939551814619705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5648202041483582"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(data.pred1,data.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.DataFrame(X_test)\n",
    "test['y']=np.array(y_test)\n",
    "test['yt']=yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4407285 0.4407284520497393\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(test.yt,test.y)),rmse(test.yt,test.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44939554"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(data.p,data.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1=['lgbm_pred', 'xg_pred', 'knn_base']\n",
    "t=data[col1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(t, y, test_size=0.05, random_state=777)\n",
    "X_train=np.array(X_train, dtype=np.float32)\n",
    "X_test=np.array(X_test, dtype=np.float32)\n",
    "#X_val=np.array(X_val, dtype=np.float32)\n",
    "\n",
    "y_train=np.array(y_train, dtype=np.float32)\n",
    "y_test=np.array(y_test, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.Sequential()\n",
    "m.add(layers.Dense( 64, input_dim=3))\n",
    "m.add(LeakyReLU(alpha=0.6657566518096245))\n",
    "m.add(Dropout(0.6574717539878921))\n",
    "m.add(layers.Dense(32))\n",
    "m.add(LeakyReLU(alpha=0.7224963844503618))\n",
    "m.add(Dropout(0.6765550357645409))\n",
    "m.add(layers.Dense(1, activation='relu'))\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.001)\n",
    "\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    \n",
    "m.compile(optimizer='rmsprop',loss=rmse_keras,metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 390 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "390/390 [==============================] - 0s 322us/step - loss: 3.5403 - mean_squared_error: 12.8903 - val_loss: 1.9475 - val_mean_squared_error: 3.8429\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 0s 257us/step - loss: 2.7810 - mean_squared_error: 7.8667 - val_loss: 1.4863 - val_mean_squared_error: 2.2518\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 0s 269us/step - loss: 2.7125 - mean_squared_error: 7.5033 - val_loss: 1.1084 - val_mean_squared_error: 1.2604\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 0s 254us/step - loss: 2.5061 - mean_squared_error: 6.4962 - val_loss: 1.0059 - val_mean_squared_error: 1.0394\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 2.3491 - mean_squared_error: 5.6672 - val_loss: 1.0142 - val_mean_squared_error: 1.0564\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 0s 288us/step - loss: 2.2402 - mean_squared_error: 5.1707 - val_loss: 1.1212 - val_mean_squared_error: 1.2887\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 0s 255us/step - loss: 2.1087 - mean_squared_error: 4.5916 - val_loss: 0.7357 - val_mean_squared_error: 0.5506\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 0s 276us/step - loss: 2.0774 - mean_squared_error: 4.4468 - val_loss: 0.9232 - val_mean_squared_error: 0.8746\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 0s 246us/step - loss: 1.9804 - mean_squared_error: 4.0014 - val_loss: 0.9891 - val_mean_squared_error: 1.0038\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 0s 256us/step - loss: 1.9700 - mean_squared_error: 3.9508 - val_loss: 0.7921 - val_mean_squared_error: 0.6414\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 0s 247us/step - loss: 1.8489 - mean_squared_error: 3.5427 - val_loss: 1.0970 - val_mean_squared_error: 1.2329\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 0s 262us/step - loss: 1.8003 - mean_squared_error: 3.3596 - val_loss: 0.8359 - val_mean_squared_error: 0.7158\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 0s 246us/step - loss: 1.7260 - mean_squared_error: 3.0910 - val_loss: 0.8397 - val_mean_squared_error: 0.7224\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 0s 255us/step - loss: 1.6970 - mean_squared_error: 2.9485 - val_loss: 0.7376 - val_mean_squared_error: 0.5539\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 0s 258us/step - loss: 1.7951 - mean_squared_error: 3.2926 - val_loss: 0.7509 - val_mean_squared_error: 0.5748\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 0s 281us/step - loss: 1.6476 - mean_squared_error: 2.7841 - val_loss: 0.9678 - val_mean_squared_error: 0.9603\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 0s 281us/step - loss: 1.6091 - mean_squared_error: 2.6581 - val_loss: 0.7031 - val_mean_squared_error: 0.5017\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 0s 277us/step - loss: 1.5707 - mean_squared_error: 2.5265 - val_loss: 0.7109 - val_mean_squared_error: 0.5134\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 0s 255us/step - loss: 1.6188 - mean_squared_error: 2.6964 - val_loss: 0.7286 - val_mean_squared_error: 0.5403\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 1.5587 - mean_squared_error: 2.5207 - val_loss: 0.6517 - val_mean_squared_error: 0.4287\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 0s 261us/step - loss: 1.6083 - mean_squared_error: 2.6550 - val_loss: 0.8623 - val_mean_squared_error: 0.7615\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 0s 260us/step - loss: 1.5336 - mean_squared_error: 2.4300 - val_loss: 0.6754 - val_mean_squared_error: 0.4618\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 0s 306us/step - loss: 1.5971 - mean_squared_error: 2.6329 - val_loss: 0.6342 - val_mean_squared_error: 0.4057\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 0s 246us/step - loss: 1.5214 - mean_squared_error: 2.3957 - val_loss: 0.7994 - val_mean_squared_error: 0.6535\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 0s 241us/step - loss: 1.5555 - mean_squared_error: 2.4727 - val_loss: 0.7423 - val_mean_squared_error: 0.5616\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 0s 253us/step - loss: 1.5160 - mean_squared_error: 2.3496 - val_loss: 0.8629 - val_mean_squared_error: 0.7623\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 0s 258us/step - loss: 1.3621 - mean_squared_error: 1.9067 - val_loss: 0.7844 - val_mean_squared_error: 0.6287\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 0s 288us/step - loss: 1.4494 - mean_squared_error: 2.1607 - val_loss: 0.7676 - val_mean_squared_error: 0.6014\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 0s 280us/step - loss: 1.4635 - mean_squared_error: 2.1993 - val_loss: 0.7059 - val_mean_squared_error: 0.5066\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 0s 248us/step - loss: 1.4989 - mean_squared_error: 2.3175 - val_loss: 0.6496 - val_mean_squared_error: 0.4267\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 0s 244us/step - loss: 1.4150 - mean_squared_error: 2.0593 - val_loss: 0.6355 - val_mean_squared_error: 0.4078\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 1.4964 - mean_squared_error: 2.3244 - val_loss: 0.6699 - val_mean_squared_error: 0.4549\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 0s 252us/step - loss: 1.3805 - mean_squared_error: 1.9470 - val_loss: 0.6484 - val_mean_squared_error: 0.4253\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 0s 263us/step - loss: 1.4103 - mean_squared_error: 2.0388 - val_loss: 0.6264 - val_mean_squared_error: 0.3963\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 0s 255us/step - loss: 1.4649 - mean_squared_error: 2.2796 - val_loss: 0.8413 - val_mean_squared_error: 0.7234\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 0s 253us/step - loss: 1.4535 - mean_squared_error: 2.1737 - val_loss: 0.6350 - val_mean_squared_error: 0.4074\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 0s 261us/step - loss: 1.4253 - mean_squared_error: 2.1027 - val_loss: 0.6312 - val_mean_squared_error: 0.4027\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 0s 246us/step - loss: 1.4659 - mean_squared_error: 2.2450 - val_loss: 0.7174 - val_mean_squared_error: 0.5236\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 0s 257us/step - loss: 1.4130 - mean_squared_error: 2.0505 - val_loss: 0.7598 - val_mean_squared_error: 0.5886\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 0s 244us/step - loss: 1.3615 - mean_squared_error: 1.9128 - val_loss: 0.7552 - val_mean_squared_error: 0.5814\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 0s 253us/step - loss: 1.3875 - mean_squared_error: 1.9662 - val_loss: 0.7394 - val_mean_squared_error: 0.5573\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 0s 263us/step - loss: 1.3851 - mean_squared_error: 1.9965 - val_loss: 0.6423 - val_mean_squared_error: 0.4174\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 0s 256us/step - loss: 1.3780 - mean_squared_error: 1.9865 - val_loss: 0.7461 - val_mean_squared_error: 0.5672\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 0s 258us/step - loss: 1.3497 - mean_squared_error: 1.8674 - val_loss: 0.6768 - val_mean_squared_error: 0.4649\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 0s 241us/step - loss: 1.2317 - mean_squared_error: 1.5929 - val_loss: 0.6962 - val_mean_squared_error: 0.4925\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 1.3042 - mean_squared_error: 1.7464 - val_loss: 0.8392 - val_mean_squared_error: 0.7185\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 0s 249us/step - loss: 1.2644 - mean_squared_error: 1.6167 - val_loss: 0.7575 - val_mean_squared_error: 0.5846\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 0s 265us/step - loss: 1.2479 - mean_squared_error: 1.5962 - val_loss: 0.5906 - val_mean_squared_error: 0.3520\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 0s 243us/step - loss: 1.3153 - mean_squared_error: 1.8204 - val_loss: 0.7218 - val_mean_squared_error: 0.5300\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 0s 253us/step - loss: 1.2892 - mean_squared_error: 1.7211 - val_loss: 0.6475 - val_mean_squared_error: 0.4248\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 0s 247us/step - loss: 1.2222 - mean_squared_error: 1.5378 - val_loss: 0.6004 - val_mean_squared_error: 0.3641\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 0s 240us/step - loss: 1.1996 - mean_squared_error: 1.4797 - val_loss: 0.7368 - val_mean_squared_error: 0.5522\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 1.1821 - mean_squared_error: 1.4376 - val_loss: 0.6709 - val_mean_squared_error: 0.4567\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 0s 247us/step - loss: 1.2248 - mean_squared_error: 1.5399 - val_loss: 0.5814 - val_mean_squared_error: 0.3412\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 0s 243us/step - loss: 1.2168 - mean_squared_error: 1.5214 - val_loss: 0.6637 - val_mean_squared_error: 0.4467\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 0s 256us/step - loss: 1.2165 - mean_squared_error: 1.5227 - val_loss: 0.5739 - val_mean_squared_error: 0.3326\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 0s 244us/step - loss: 1.2167 - mean_squared_error: 1.5344 - val_loss: 0.5860 - val_mean_squared_error: 0.3469\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 0s 252us/step - loss: 1.1843 - mean_squared_error: 1.4457 - val_loss: 0.6725 - val_mean_squared_error: 0.4586\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 0s 238us/step - loss: 1.2120 - mean_squared_error: 1.5038 - val_loss: 0.7072 - val_mean_squared_error: 0.5075\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 0s 249us/step - loss: 1.2048 - mean_squared_error: 1.4956 - val_loss: 0.6068 - val_mean_squared_error: 0.3723\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 0s 242us/step - loss: 1.1260 - mean_squared_error: 1.3472 - val_loss: 0.6189 - val_mean_squared_error: 0.3876\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 0s 238us/step - loss: 1.2178 - mean_squared_error: 1.5293 - val_loss: 0.6025 - val_mean_squared_error: 0.3670\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 0s 238us/step - loss: 1.1766 - mean_squared_error: 1.4376 - val_loss: 0.5850 - val_mean_squared_error: 0.3457\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 0s 245us/step - loss: 1.1488 - mean_squared_error: 1.3931 - val_loss: 0.5792 - val_mean_squared_error: 0.3387\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 0s 248us/step - loss: 1.1792 - mean_squared_error: 1.4454 - val_loss: 0.5958 - val_mean_squared_error: 0.3588\n",
      "Epoch 66/100\n",
      "390/390 [==============================] - 0s 242us/step - loss: 1.1499 - mean_squared_error: 1.3823 - val_loss: 0.5198 - val_mean_squared_error: 0.2731\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 1.1219 - mean_squared_error: 1.3008 - val_loss: 0.5725 - val_mean_squared_error: 0.3309\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 0s 256us/step - loss: 1.1424 - mean_squared_error: 1.3581 - val_loss: 0.5183 - val_mean_squared_error: 0.2724\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 0s 246us/step - loss: 1.1689 - mean_squared_error: 1.4011 - val_loss: 0.6686 - val_mean_squared_error: 0.4528\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 1.1460 - mean_squared_error: 1.3854 - val_loss: 0.6122 - val_mean_squared_error: 0.3791\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 0s 243us/step - loss: 1.1808 - mean_squared_error: 1.4282 - val_loss: 0.6027 - val_mean_squared_error: 0.3673\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 0s 245us/step - loss: 1.1367 - mean_squared_error: 1.3612 - val_loss: 0.6432 - val_mean_squared_error: 0.4187\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 0s 238us/step - loss: 1.1725 - mean_squared_error: 1.4248 - val_loss: 0.5080 - val_mean_squared_error: 0.2615\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 0s 248us/step - loss: 1.1302 - mean_squared_error: 1.3307 - val_loss: 0.5767 - val_mean_squared_error: 0.3358\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 0s 245us/step - loss: 1.0543 - mean_squared_error: 1.1535 - val_loss: 0.5662 - val_mean_squared_error: 0.3236\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 0s 240us/step - loss: 1.1998 - mean_squared_error: 1.4790 - val_loss: 0.5369 - val_mean_squared_error: 0.2910\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 0s 241us/step - loss: 1.0557 - mean_squared_error: 1.1658 - val_loss: 0.5356 - val_mean_squared_error: 0.2897\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 0s 241us/step - loss: 1.1085 - mean_squared_error: 1.2867 - val_loss: 0.5425 - val_mean_squared_error: 0.2970\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 0s 241us/step - loss: 1.0914 - mean_squared_error: 1.2377 - val_loss: 0.5563 - val_mean_squared_error: 0.3123\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 0s 238us/step - loss: 0.9855 - mean_squared_error: 0.9981 - val_loss: 0.5261 - val_mean_squared_error: 0.2794\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 0s 249us/step - loss: 1.1043 - mean_squared_error: 1.2591 - val_loss: 0.5617 - val_mean_squared_error: 0.3185\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 0s 240us/step - loss: 1.1078 - mean_squared_error: 1.2797 - val_loss: 0.4980 - val_mean_squared_error: 0.2511\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 0s 253us/step - loss: 1.0421 - mean_squared_error: 1.1225 - val_loss: 0.6184 - val_mean_squared_error: 0.3864\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 1.0640 - mean_squared_error: 1.1585 - val_loss: 0.5038 - val_mean_squared_error: 0.2563\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 0s 236us/step - loss: 1.0268 - mean_squared_error: 1.0916 - val_loss: 0.5203 - val_mean_squared_error: 0.2732\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 0s 252us/step - loss: 1.0127 - mean_squared_error: 1.0712 - val_loss: 0.5751 - val_mean_squared_error: 0.3339\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 0s 238us/step - loss: 1.0654 - mean_squared_error: 1.1495 - val_loss: 0.4960 - val_mean_squared_error: 0.2494\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 0s 248us/step - loss: 1.1234 - mean_squared_error: 1.3070 - val_loss: 0.6319 - val_mean_squared_error: 0.4037\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 0s 241us/step - loss: 1.0627 - mean_squared_error: 1.1482 - val_loss: 0.5732 - val_mean_squared_error: 0.3318\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 0s 260us/step - loss: 1.0435 - mean_squared_error: 1.1198 - val_loss: 0.5146 - val_mean_squared_error: 0.2673\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 0s 294us/step - loss: 0.9722 - mean_squared_error: 0.9712 - val_loss: 0.5656 - val_mean_squared_error: 0.3228\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 0s 288us/step - loss: 0.9986 - mean_squared_error: 1.0304 - val_loss: 0.5544 - val_mean_squared_error: 0.3102\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 0s 251us/step - loss: 1.0433 - mean_squared_error: 1.1145 - val_loss: 0.5272 - val_mean_squared_error: 0.2807\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 0s 296us/step - loss: 0.9835 - mean_squared_error: 0.9919 - val_loss: 0.5065 - val_mean_squared_error: 0.2598\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 0s 286us/step - loss: 1.0266 - mean_squared_error: 1.0912 - val_loss: 0.5140 - val_mean_squared_error: 0.2671\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 0s 284us/step - loss: 1.0196 - mean_squared_error: 1.1033 - val_loss: 0.6470 - val_mean_squared_error: 0.4228\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 0s 293us/step - loss: 0.9657 - mean_squared_error: 0.9613 - val_loss: 0.5205 - val_mean_squared_error: 0.2737\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 0s 288us/step - loss: 0.9618 - mean_squared_error: 0.9562 - val_loss: 0.5331 - val_mean_squared_error: 0.2869\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 0s 256us/step - loss: 1.0604 - mean_squared_error: 1.1582 - val_loss: 0.6406 - val_mean_squared_error: 0.4144\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 0s 261us/step - loss: 0.9637 - mean_squared_error: 0.9480 - val_loss: 0.6595 - val_mean_squared_error: 0.4394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22158ca6390>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.25, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input835\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    }
   ],
   "source": [
    "data['ym']=np.clip(m.predict(t),y.min(),y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6513185268453074"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(data.ym,data.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_item</th>\n",
       "      <th>lgbm_pred</th>\n",
       "      <th>xg_pred</th>\n",
       "      <th>knn_base</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WI_000165</td>\n",
       "      <td>2.898725</td>\n",
       "      <td>3.587146</td>\n",
       "      <td>2.939409</td>\n",
       "      <td>5.768516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WI_000196</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>3.465379</td>\n",
       "      <td>2.959683</td>\n",
       "      <td>5.698601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WI_000264</td>\n",
       "      <td>2.898725</td>\n",
       "      <td>4.422327</td>\n",
       "      <td>2.939391</td>\n",
       "      <td>6.223756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WI_000273</td>\n",
       "      <td>2.946769</td>\n",
       "      <td>3.219804</td>\n",
       "      <td>2.940727</td>\n",
       "      <td>5.584076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WI_000278</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>3.692698</td>\n",
       "      <td>2.956984</td>\n",
       "      <td>5.819813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_item  lgbm_pred   xg_pred  knn_base      pred\n",
       "0  WI_000165   2.898725  3.587146  2.939409  5.768516\n",
       "1  WI_000196   2.819722  3.465379  2.959683  5.698601\n",
       "2  WI_000264   2.898725  4.422327  2.939391  6.223756\n",
       "3  WI_000273   2.946769  3.219804  2.940727  5.584076\n",
       "4  WI_000278   2.819722  3.692698  2.956984  5.819813"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub2=pd.read_csv(r'C:/Users/Abhishek/Downloads/open_times_submission2_for_keras.csv')\n",
    "sub2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['lgbm_pred', 'xg_pred', 'knn_base', 'pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input4\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    }
   ],
   "source": [
    "sub2['dnn_pred']=model.predict(sub2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2.to_csv(path_or_buf=r'C:\\Users\\Abhishek\\Downloads\\sub2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_item</th>\n",
       "      <th>lgbm_pred</th>\n",
       "      <th>xg_pred</th>\n",
       "      <th>knn_base</th>\n",
       "      <th>pred</th>\n",
       "      <th>dnn_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WI_000165</td>\n",
       "      <td>2.898725</td>\n",
       "      <td>3.587146</td>\n",
       "      <td>2.939409</td>\n",
       "      <td>5.768516</td>\n",
       "      <td>6.311718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WI_000196</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>3.465379</td>\n",
       "      <td>2.959683</td>\n",
       "      <td>5.698601</td>\n",
       "      <td>6.392498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WI_000264</td>\n",
       "      <td>2.898725</td>\n",
       "      <td>4.422327</td>\n",
       "      <td>2.939391</td>\n",
       "      <td>6.223756</td>\n",
       "      <td>6.546648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WI_000273</td>\n",
       "      <td>2.946769</td>\n",
       "      <td>3.219804</td>\n",
       "      <td>2.940727</td>\n",
       "      <td>5.584076</td>\n",
       "      <td>6.159549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WI_000278</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>3.692698</td>\n",
       "      <td>2.956984</td>\n",
       "      <td>5.819813</td>\n",
       "      <td>6.452962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WI_000298</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>4.077368</td>\n",
       "      <td>2.952962</td>\n",
       "      <td>6.025475</td>\n",
       "      <td>6.553432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WI_000396</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>4.084836</td>\n",
       "      <td>2.960943</td>\n",
       "      <td>6.037527</td>\n",
       "      <td>6.571674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WI_000469</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>3.309238</td>\n",
       "      <td>2.961317</td>\n",
       "      <td>5.615122</td>\n",
       "      <td>6.349791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WI_000492</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>4.078639</td>\n",
       "      <td>2.948484</td>\n",
       "      <td>6.021690</td>\n",
       "      <td>6.544733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WI_000502</td>\n",
       "      <td>3.105749</td>\n",
       "      <td>3.606221</td>\n",
       "      <td>2.927764</td>\n",
       "      <td>5.829671</td>\n",
       "      <td>6.095752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WI_000512</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>3.793433</td>\n",
       "      <td>2.957661</td>\n",
       "      <td>5.875401</td>\n",
       "      <td>6.483022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WI_000583</td>\n",
       "      <td>3.030933</td>\n",
       "      <td>3.458043</td>\n",
       "      <td>2.950964</td>\n",
       "      <td>5.749547</td>\n",
       "      <td>6.170467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WI_000614</td>\n",
       "      <td>3.237958</td>\n",
       "      <td>3.739270</td>\n",
       "      <td>2.949314</td>\n",
       "      <td>5.963596</td>\n",
       "      <td>6.051422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WI_000641</td>\n",
       "      <td>2.990627</td>\n",
       "      <td>1.692842</td>\n",
       "      <td>2.905117</td>\n",
       "      <td>4.729338</td>\n",
       "      <td>5.599421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WI_000771</td>\n",
       "      <td>3.189913</td>\n",
       "      <td>3.308879</td>\n",
       "      <td>2.910546</td>\n",
       "      <td>5.675741</td>\n",
       "      <td>5.892154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WI_000825</td>\n",
       "      <td>3.189913</td>\n",
       "      <td>3.441239</td>\n",
       "      <td>2.893548</td>\n",
       "      <td>5.730892</td>\n",
       "      <td>5.897861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WI_000847</td>\n",
       "      <td>3.153793</td>\n",
       "      <td>2.956586</td>\n",
       "      <td>2.895049</td>\n",
       "      <td>5.457321</td>\n",
       "      <td>5.791503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WI_000871</td>\n",
       "      <td>2.867766</td>\n",
       "      <td>3.577223</td>\n",
       "      <td>2.872731</td>\n",
       "      <td>5.687097</td>\n",
       "      <td>6.204236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WI_000872</td>\n",
       "      <td>2.867766</td>\n",
       "      <td>3.577223</td>\n",
       "      <td>2.872601</td>\n",
       "      <td>5.686967</td>\n",
       "      <td>6.203974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WI_000873</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>3.557100</td>\n",
       "      <td>2.868458</td>\n",
       "      <td>5.657373</td>\n",
       "      <td>6.235720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WI_000891</td>\n",
       "      <td>3.105749</td>\n",
       "      <td>2.748977</td>\n",
       "      <td>2.915316</td>\n",
       "      <td>5.349939</td>\n",
       "      <td>5.816677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>WI_000931</td>\n",
       "      <td>2.819722</td>\n",
       "      <td>3.216051</td>\n",
       "      <td>2.856723</td>\n",
       "      <td>5.459732</td>\n",
       "      <td>6.112196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>WI_000963</td>\n",
       "      <td>3.030933</td>\n",
       "      <td>4.596827</td>\n",
       "      <td>2.954682</td>\n",
       "      <td>6.374018</td>\n",
       "      <td>6.498698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>WI_001018</td>\n",
       "      <td>2.867766</td>\n",
       "      <td>4.417937</td>\n",
       "      <td>2.827986</td>\n",
       "      <td>6.100626</td>\n",
       "      <td>6.347106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>WI_001022</td>\n",
       "      <td>2.982889</td>\n",
       "      <td>4.308572</td>\n",
       "      <td>2.995995</td>\n",
       "      <td>6.243721</td>\n",
       "      <td>6.549623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>WI_001031</td>\n",
       "      <td>3.189913</td>\n",
       "      <td>3.489001</td>\n",
       "      <td>3.016065</td>\n",
       "      <td>5.879444</td>\n",
       "      <td>6.157460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>WI_001037</td>\n",
       "      <td>2.946769</td>\n",
       "      <td>2.959803</td>\n",
       "      <td>3.011466</td>\n",
       "      <td>5.513089</td>\n",
       "      <td>6.224267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>WI_001043</td>\n",
       "      <td>3.237958</td>\n",
       "      <td>3.900212</td>\n",
       "      <td>3.022734</td>\n",
       "      <td>6.124747</td>\n",
       "      <td>6.245712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>WI_001068</td>\n",
       "      <td>3.189913</td>\n",
       "      <td>3.942694</td>\n",
       "      <td>3.031607</td>\n",
       "      <td>6.142295</td>\n",
       "      <td>6.321553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>WI_001081</td>\n",
       "      <td>2.898725</td>\n",
       "      <td>3.305943</td>\n",
       "      <td>3.018480</td>\n",
       "      <td>5.694302</td>\n",
       "      <td>6.387358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>WI_011534</td>\n",
       "      <td>3.215058</td>\n",
       "      <td>0.702674</td>\n",
       "      <td>2.212568</td>\n",
       "      <td>3.564697</td>\n",
       "      <td>3.675870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>WI_011535</td>\n",
       "      <td>3.215058</td>\n",
       "      <td>0.702674</td>\n",
       "      <td>2.214667</td>\n",
       "      <td>3.566796</td>\n",
       "      <td>3.679968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>WI_011536</td>\n",
       "      <td>3.263103</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>2.216793</td>\n",
       "      <td>3.660132</td>\n",
       "      <td>3.688311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>WI_011537</td>\n",
       "      <td>3.215058</td>\n",
       "      <td>0.625078</td>\n",
       "      <td>2.198763</td>\n",
       "      <td>3.508594</td>\n",
       "      <td>3.621284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>WI_011538</td>\n",
       "      <td>3.215058</td>\n",
       "      <td>0.625078</td>\n",
       "      <td>2.199356</td>\n",
       "      <td>3.509187</td>\n",
       "      <td>3.622443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>WI_011539</td>\n",
       "      <td>3.140243</td>\n",
       "      <td>2.293007</td>\n",
       "      <td>2.226768</td>\n",
       "      <td>4.423238</td>\n",
       "      <td>4.280350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>WI_011540</td>\n",
       "      <td>3.215058</td>\n",
       "      <td>0.785961</td>\n",
       "      <td>2.198536</td>\n",
       "      <td>3.596065</td>\n",
       "      <td>3.677911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>WI_011541</td>\n",
       "      <td>3.299223</td>\n",
       "      <td>1.669792</td>\n",
       "      <td>2.200143</td>\n",
       "      <td>4.104818</td>\n",
       "      <td>3.895504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>WI_011542</td>\n",
       "      <td>3.255365</td>\n",
       "      <td>2.490138</td>\n",
       "      <td>2.234802</td>\n",
       "      <td>4.573428</td>\n",
       "      <td>4.250271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>WI_011543</td>\n",
       "      <td>3.462390</td>\n",
       "      <td>2.272128</td>\n",
       "      <td>2.229745</td>\n",
       "      <td>4.511937</td>\n",
       "      <td>3.989167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>WI_011545</td>\n",
       "      <td>3.219245</td>\n",
       "      <td>2.229700</td>\n",
       "      <td>2.237350</td>\n",
       "      <td>4.423124</td>\n",
       "      <td>4.209583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>WI_011546</td>\n",
       "      <td>3.426270</td>\n",
       "      <td>2.586115</td>\n",
       "      <td>2.229578</td>\n",
       "      <td>4.672037</td>\n",
       "      <td>4.117319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>WI_011547</td>\n",
       "      <td>3.378225</td>\n",
       "      <td>2.353008</td>\n",
       "      <td>2.230194</td>\n",
       "      <td>4.531104</td>\n",
       "      <td>4.089191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>WI_011548</td>\n",
       "      <td>3.510434</td>\n",
       "      <td>2.283203</td>\n",
       "      <td>2.231232</td>\n",
       "      <td>4.533942</td>\n",
       "      <td>3.953307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>WI_011549</td>\n",
       "      <td>3.255365</td>\n",
       "      <td>2.564355</td>\n",
       "      <td>2.232744</td>\n",
       "      <td>4.611826</td>\n",
       "      <td>4.268964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>WI_011550</td>\n",
       "      <td>3.215058</td>\n",
       "      <td>2.336855</td>\n",
       "      <td>2.212037</td>\n",
       "      <td>4.454960</td>\n",
       "      <td>4.196585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>WI_011551</td>\n",
       "      <td>3.171201</td>\n",
       "      <td>2.275146</td>\n",
       "      <td>2.235737</td>\n",
       "      <td>4.431802</td>\n",
       "      <td>4.264046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>WI_011552</td>\n",
       "      <td>3.140243</td>\n",
       "      <td>3.665477</td>\n",
       "      <td>2.224842</td>\n",
       "      <td>5.169447</td>\n",
       "      <td>4.677005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>WI_011553</td>\n",
       "      <td>3.008034</td>\n",
       "      <td>3.017345</td>\n",
       "      <td>2.214296</td>\n",
       "      <td>4.765752</td>\n",
       "      <td>4.594846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>WI_011554</td>\n",
       "      <td>3.171201</td>\n",
       "      <td>2.043772</td>\n",
       "      <td>2.232551</td>\n",
       "      <td>4.302493</td>\n",
       "      <td>4.188361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>WI_011555</td>\n",
       "      <td>3.255365</td>\n",
       "      <td>2.531227</td>\n",
       "      <td>2.235737</td>\n",
       "      <td>4.596761</td>\n",
       "      <td>4.264637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>WI_011556</td>\n",
       "      <td>3.299223</td>\n",
       "      <td>2.867791</td>\n",
       "      <td>2.201032</td>\n",
       "      <td>4.758737</td>\n",
       "      <td>4.264148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>WI_011557</td>\n",
       "      <td>3.171201</td>\n",
       "      <td>2.865328</td>\n",
       "      <td>2.238839</td>\n",
       "      <td>4.756613</td>\n",
       "      <td>4.448585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>WI_011558</td>\n",
       "      <td>3.219245</td>\n",
       "      <td>2.505717</td>\n",
       "      <td>2.237339</td>\n",
       "      <td>4.573570</td>\n",
       "      <td>4.292506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>WI_011559</td>\n",
       "      <td>3.171201</td>\n",
       "      <td>2.704445</td>\n",
       "      <td>2.234802</td>\n",
       "      <td>4.664878</td>\n",
       "      <td>4.391493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>WI_011560</td>\n",
       "      <td>3.215058</td>\n",
       "      <td>2.812969</td>\n",
       "      <td>2.216550</td>\n",
       "      <td>4.719003</td>\n",
       "      <td>4.350564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>WI_011561</td>\n",
       "      <td>3.215058</td>\n",
       "      <td>2.812969</td>\n",
       "      <td>2.218702</td>\n",
       "      <td>4.721155</td>\n",
       "      <td>4.354632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>WI_011562</td>\n",
       "      <td>3.303409</td>\n",
       "      <td>2.795523</td>\n",
       "      <td>2.236115</td>\n",
       "      <td>4.755689</td>\n",
       "      <td>4.303899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>WI_011563</td>\n",
       "      <td>3.056078</td>\n",
       "      <td>2.547930</td>\n",
       "      <td>2.197106</td>\n",
       "      <td>4.507166</td>\n",
       "      <td>4.375647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>WI_011564</td>\n",
       "      <td>3.263103</td>\n",
       "      <td>1.922769</td>\n",
       "      <td>2.192561</td>\n",
       "      <td>4.224247</td>\n",
       "      <td>3.989964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      work_item  lgbm_pred   xg_pred  knn_base      pred  dnn_pred\n",
       "0     WI_000165   2.898725  3.587146  2.939409  5.768516  6.311718\n",
       "1     WI_000196   2.819722  3.465379  2.959683  5.698601  6.392498\n",
       "2     WI_000264   2.898725  4.422327  2.939391  6.223756  6.546648\n",
       "3     WI_000273   2.946769  3.219804  2.940727  5.584076  6.159549\n",
       "4     WI_000278   2.819722  3.692698  2.956984  5.819813  6.452962\n",
       "5     WI_000298   2.819722  4.077368  2.952962  6.025475  6.553432\n",
       "6     WI_000396   2.819722  4.084836  2.960943  6.037527  6.571674\n",
       "7     WI_000469   2.819722  3.309238  2.961317  5.615122  6.349791\n",
       "8     WI_000492   2.819722  4.078639  2.948484  6.021690  6.544733\n",
       "9     WI_000502   3.105749  3.606221  2.927764  5.829671  6.095752\n",
       "10    WI_000512   2.819722  3.793433  2.957661  5.875401  6.483022\n",
       "11    WI_000583   3.030933  3.458043  2.950964  5.749547  6.170467\n",
       "12    WI_000614   3.237958  3.739270  2.949314  5.963596  6.051422\n",
       "13    WI_000641   2.990627  1.692842  2.905117  4.729338  5.599421\n",
       "14    WI_000771   3.189913  3.308879  2.910546  5.675741  5.892154\n",
       "15    WI_000825   3.189913  3.441239  2.893548  5.730892  5.897861\n",
       "16    WI_000847   3.153793  2.956586  2.895049  5.457321  5.791503\n",
       "17    WI_000871   2.867766  3.577223  2.872731  5.687097  6.204236\n",
       "18    WI_000872   2.867766  3.577223  2.872601  5.686967  6.203974\n",
       "19    WI_000873   2.819722  3.557100  2.868458  5.657373  6.235720\n",
       "20    WI_000891   3.105749  2.748977  2.915316  5.349939  5.816677\n",
       "21    WI_000931   2.819722  3.216051  2.856723  5.459732  6.112196\n",
       "22    WI_000963   3.030933  4.596827  2.954682  6.374018  6.498698\n",
       "23    WI_001018   2.867766  4.417937  2.827986  6.100626  6.347106\n",
       "24    WI_001022   2.982889  4.308572  2.995995  6.243721  6.549623\n",
       "25    WI_001031   3.189913  3.489001  3.016065  5.879444  6.157460\n",
       "26    WI_001037   2.946769  2.959803  3.011466  5.513089  6.224267\n",
       "27    WI_001043   3.237958  3.900212  3.022734  6.124747  6.245712\n",
       "28    WI_001068   3.189913  3.942694  3.031607  6.142295  6.321553\n",
       "29    WI_001081   2.898725  3.305943  3.018480  5.694302  6.387358\n",
       "...         ...        ...       ...       ...       ...       ...\n",
       "1012  WI_011534   3.215058  0.702674  2.212568  3.564697  3.675870\n",
       "1013  WI_011535   3.215058  0.702674  2.214667  3.566796  3.679968\n",
       "1014  WI_011536   3.263103  0.843434  2.216793  3.660132  3.688311\n",
       "1015  WI_011537   3.215058  0.625078  2.198763  3.508594  3.621284\n",
       "1016  WI_011538   3.215058  0.625078  2.199356  3.509187  3.622443\n",
       "1017  WI_011539   3.140243  2.293007  2.226768  4.423238  4.280350\n",
       "1018  WI_011540   3.215058  0.785961  2.198536  3.596065  3.677911\n",
       "1019  WI_011541   3.299223  1.669792  2.200143  4.104818  3.895504\n",
       "1020  WI_011542   3.255365  2.490138  2.234802  4.573428  4.250271\n",
       "1021  WI_011543   3.462390  2.272128  2.229745  4.511937  3.989167\n",
       "1022  WI_011545   3.219245  2.229700  2.237350  4.423124  4.209583\n",
       "1023  WI_011546   3.426270  2.586115  2.229578  4.672037  4.117319\n",
       "1024  WI_011547   3.378225  2.353008  2.230194  4.531104  4.089191\n",
       "1025  WI_011548   3.510434  2.283203  2.231232  4.533942  3.953307\n",
       "1026  WI_011549   3.255365  2.564355  2.232744  4.611826  4.268964\n",
       "1027  WI_011550   3.215058  2.336855  2.212037  4.454960  4.196585\n",
       "1028  WI_011551   3.171201  2.275146  2.235737  4.431802  4.264046\n",
       "1029  WI_011552   3.140243  3.665477  2.224842  5.169447  4.677005\n",
       "1030  WI_011553   3.008034  3.017345  2.214296  4.765752  4.594846\n",
       "1031  WI_011554   3.171201  2.043772  2.232551  4.302493  4.188361\n",
       "1032  WI_011555   3.255365  2.531227  2.235737  4.596761  4.264637\n",
       "1033  WI_011556   3.299223  2.867791  2.201032  4.758737  4.264148\n",
       "1034  WI_011557   3.171201  2.865328  2.238839  4.756613  4.448585\n",
       "1035  WI_011558   3.219245  2.505717  2.237339  4.573570  4.292506\n",
       "1036  WI_011559   3.171201  2.704445  2.234802  4.664878  4.391493\n",
       "1037  WI_011560   3.215058  2.812969  2.216550  4.719003  4.350564\n",
       "1038  WI_011561   3.215058  2.812969  2.218702  4.721155  4.354632\n",
       "1039  WI_011562   3.303409  2.795523  2.236115  4.755689  4.303899\n",
       "1040  WI_011563   3.056078  2.547930  2.197106  4.507166  4.375647\n",
       "1041  WI_011564   3.263103  1.922769  2.192561  4.224247  3.989964\n",
       "\n",
       "[1042 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
